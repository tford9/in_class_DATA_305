{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Simple RAG (Retrieval-Augmented Generation) with Gemini\n\n**DATA 305 - Module 2: Prompt Pipelines and Structured Reasoning**\n\nThis notebook demonstrates a minimal RAG pipeline using `google-genai`, `sentence-transformers`, and `numpy` — no LangChain, no vector databases. By the end, you'll understand the core mechanics behind every RAG system:\n\n1. **Chunk** a document into passages\n2. **Embed** each chunk into a vector (using sentence-transformers)\n3. **Retrieve** the most relevant chunks for a query\n4. **Generate** a grounded answer using retrieved context (using Gemini)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We only need two packages. If you're running in Colab, install `google-genai` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q google-genai sentence-transformers numpy"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your own API key from Google AI Studio\n",
    "my_api_key = \"YOUR_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google import genai\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\n\nclient = genai.Client(api_key=my_api_key)\n\n# Load a lightweight sentence-transformers model for embeddings\nembedder = SentenceTransformer(\"all-MiniLM-L6-v2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Chunk a Document\n",
    "\n",
    "RAG starts with **chunking**: splitting a document into smaller passages so we can retrieve only the relevant parts later.\n",
    "\n",
    "Here we'll use a sample document defined as a string. In practice, you'd load a file from disk or the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample document about William & Mary (feel free to replace with your own text!)\n",
    "document = \"\"\"\n",
    "The College of William & Mary, founded in 1693, is the second-oldest institution of higher\n",
    "education in the United States. It was established by a royal charter issued by King William III\n",
    "and Queen Mary II of England. The college is located in Williamsburg, Virginia.\n",
    "\n",
    "William & Mary has a strong liberal arts tradition and is classified as a Public Ivy. The\n",
    "university enrolls approximately 6,200 undergraduate students and 2,500 graduate students.\n",
    "It is known for its rigorous academic programs, particularly in government, business, law,\n",
    "and the sciences.\n",
    "\n",
    "The campus features the Sir Christopher Wren Building, which is the oldest academic building\n",
    "still in continuous use in the United States. The building was constructed between 1695 and 1700\n",
    "and has survived three fires over its history. It originally housed the entire college including\n",
    "classrooms, living quarters, and a chapel.\n",
    "\n",
    "William & Mary has produced many notable alumni, including three U.S. Presidents: Thomas\n",
    "Jefferson, James Monroe, and John Tyler. Other famous alumni include U.S. Supreme Court Chief\n",
    "Justice John Marshall and comedian Jon Stewart. The university's motto is \"Hark Upon the Gale.\"\n",
    "\n",
    "The William & Mary Tribe competes in NCAA Division I athletics as a member of the Coastal\n",
    "Athletic Association. The school colors are green, gold, and silver. Popular sports include\n",
    "football, basketball, and gymnastics. The Tribe football team plays at Zable Stadium.\n",
    "\n",
    "The university is home to several research centers, including the Virginia Institute of Marine\n",
    "Science (VIMS), one of the largest marine science research and education centers in the country.\n",
    "VIMS is located on the York River in Gloucester Point, Virginia, and conducts research on\n",
    "coastal and marine ecosystems.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks by character count.\n",
    "    Overlap helps ensure we don't cut important context at chunk boundaries.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk.strip())\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Try a paragraph-based chunking approach instead — split on double newlines\n",
    "def chunk_by_paragraph(text):\n",
    "    \"\"\"Split text into chunks based on paragraph boundaries.\"\"\"\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "chunks = chunk_by_paragraph(document)\n",
    "\n",
    "print(f\"Document split into {len(chunks)} chunks:\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:100] + \"...\" if len(chunk) > 100 else chunk)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 2: Compute Embeddings\n\nAn **embedding** is a numerical vector that captures the meaning of a piece of text. Similar texts produce vectors that are close together in vector space.\n\nWe'll use the `all-MiniLM-L6-v2` model from [sentence-transformers](https://www.sbert.net/) to embed each chunk. This model runs locally (no API key needed for embeddings!) and produces 384-dimensional vectors."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_embeddings(texts):\n    \"\"\"Get embeddings for a list of texts using sentence-transformers.\"\"\"\n    return embedder.encode(texts, convert_to_numpy=True)\n\n\n# Embed all chunks\nchunk_embeddings = get_embeddings(chunks)\n\nprint(f\"Each embedding has {chunk_embeddings[0].shape[0]} dimensions\")\nprint(f\"First embedding (first 10 values): {chunk_embeddings[0][:10]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Retrieve Relevant Chunks\n",
    "\n",
    "Given a user query, we:\n",
    "1. Embed the query using the same model\n",
    "2. Compute **cosine similarity** between the query embedding and each chunk embedding\n",
    "3. Return the top-k most similar chunks\n",
    "\n",
    "**Cosine similarity** measures the angle between two vectors — values close to 1.0 mean the texts are semantically similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def cosine_similarity(a, b):\n    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n\ndef retrieve(query, chunks, chunk_embeddings, top_k=2):\n    \"\"\"Find the top_k most relevant chunks for a query.\"\"\"\n    query_embedding = get_embeddings([query])[0]\n\n    similarities = [\n        cosine_similarity(query_embedding, chunk_emb)\n        for chunk_emb in chunk_embeddings\n    ]\n\n    # Get indices of top-k chunks sorted by similarity (highest first)\n    top_indices = np.argsort(similarities)[::-1][:top_k]\n\n    results = []\n    for idx in top_indices:\n        results.append({\n            \"chunk_index\": int(idx),\n            \"similarity\": float(similarities[idx]),\n            \"text\": chunks[idx],\n        })\n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a retrieval query\n",
    "query = \"What presidents went to William & Mary?\"\n",
    "results = retrieve(query, chunks, chunk_embeddings, top_k=2)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "for r in results:\n",
    "    print(f\"Chunk {r['chunk_index']} (similarity: {r['similarity']:.4f}):\")\n",
    "    print(r[\"text\"])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate a Grounded Answer\n",
    "\n",
    "Now we combine retrieval with generation. We pass the retrieved chunks as **context** in the prompt, asking Gemini to answer based only on the provided information.\n",
    "\n",
    "This is the key idea of RAG: the model generates answers **grounded** in retrieved evidence rather than relying solely on its training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query, chunks, chunk_embeddings, top_k=2):\n",
    "    \"\"\"Full RAG pipeline: retrieve relevant chunks, then generate an answer.\"\"\"\n",
    "\n",
    "    # Step 1: Retrieve\n",
    "    retrieved = retrieve(query, chunks, chunk_embeddings, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([r[\"text\"] for r in retrieved])\n",
    "\n",
    "    # Step 2: Generate with context\n",
    "    prompt = f\"\"\"Answer the following question using ONLY the provided context.\n",
    "If the context doesn't contain enough information, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=prompt,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"answer\": response.text,\n",
    "        \"retrieved_chunks\": retrieved,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question!\n",
    "result = rag_query(\"What presidents went to William & Mary?\", chunks, chunk_embeddings)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\n--- Retrieved Context ---\")\n",
    "for r in result[\"retrieved_chunks\"]:\n",
    "    print(f\"\\nChunk {r['chunk_index']} (similarity: {r['similarity']:.4f}):\")\n",
    "    print(r[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "result = rag_query(\"Tell me about marine science research.\", chunks, chunk_embeddings)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\n--- Retrieved Context ---\")\n",
    "for r in result[\"retrieved_chunks\"]:\n",
    "    print(f\"\\nChunk {r['chunk_index']} (similarity: {r['similarity']:.4f}):\")\n",
    "    print(r[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a question the document can't answer\n",
    "result = rag_query(\"What is the tuition at William & Mary?\", chunks, chunk_embeddings)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\n--- Retrieved Context ---\")\n",
    "for r in result[\"retrieved_chunks\"]:\n",
    "    print(f\"\\nChunk {r['chunk_index']} (similarity: {r['similarity']:.4f}):\")\n",
    "    print(r[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. **Try your own document:** Replace the `document` string with text from a Wikipedia article, a course syllabus, or any other source. How well does retrieval work?\n",
    "\n",
    "2. **Experiment with chunking:** Try `chunk_text()` with different `chunk_size` and `overlap` values instead of `chunk_by_paragraph()`. How does chunk size affect retrieval quality?\n",
    "\n",
    "3. **Change top_k:** What happens when you retrieve more or fewer chunks? Try `top_k=1` vs `top_k=4`.\n",
    "\n",
    "4. **Improve the prompt:** Modify the generation prompt in `rag_query()`. Can you get the model to cite which chunk it used? Can you get it to respond in a specific format?"
   ]
  }
 ]
}